## **8.2.** **回退消息**

### **8.2.1.** **Mandatory 参数** 

**在仅开启了生产者确认机制的情况下，交换机接收到消息后，会直接给消息生产者发送确认消息**，**如果发现该消息不可路由，那么消息会被直接丢弃，此时生产者是不知道消息被丢弃这个事件的**。那么如何让无法被路由的消息帮我想办法处理一下？最起码通知我一声，我好自己处理啊。通过设置 mandatory 参数可以在当消息传递过程中不可达目的地时将消息返回给生产者。

**`开启回退配置`**

```text
spring.rabbitmq.publisher-returns=true
```

### **8.2.2.** **消息生产者代码** 

```java
@Slf4j
@RestController
@RequestMapping("/confirm")
public class ProducerController {

    @Autowired
    private RabbitTemplate rabbitTemplate;

    @GetMapping("/sendConfirmMsg/{message}")
    public void sendConfirmMsg(@PathVariable("message") String message) {
        CorrelationData correlationData = new CorrelationData("1");

        // 正确路由消息
        rabbitTemplate.convertAndSend(
                ConfirmConfig.CONFIRM_EXCHANGE_NAME,
                ConfirmConfig.CONFIRM_ROUTE_KEY,
                message,
                correlationData
        );
        // 错误路由消息
        rabbitTemplate.convertAndSend(
                ConfirmConfig.CONFIRM_EXCHANGE_NAME,
                ConfirmConfig.CONFIRM_ROUTE_KEY + "fake",
                message,
                correlationData
        );
        log.info("Sending confirm Message : " + message);
    }
}
```

### 8.2.3. 回调接口

**<font color="000bb"> 千万别忘了把回退接口也注入回rabbitTemplate对象</font>**

```java
@Slf4j
@Component
public class MyCallBack implements RabbitTemplate.ConfirmCallback, RabbitTemplate.ReturnsCallback {

    @Autowired
    private RabbitTemplate rabbitTemplate;

    // 把内部接口实现类注入
    @PostConstruct
    public void init() {
        rabbitTemplate.setConfirmCallback(this);
        rabbitTemplate.setReturnsCallback(this);
    }

    /**
     * @param correlationData 保存回调的消息的ID和相关信息
     * @param ack             true for ack, false for nack
     * @param cause           An optional cause, for nack, when available, otherwise null.
     */
    @Override
    public void confirm(CorrelationData correlationData, boolean ack, String cause) {
        String id = correlationData != null ? correlationData.getId() : "";
        if (ack) {
            log.info("交换机收到了ID为:{} 的消息", id);
        } else {
            log.info("交换机收到了ID为:{} 的消息, 原因为 :{}", id, cause);
        }
    }

    @Override
    public void returnedMessage(ReturnedMessage returned) {
        log.info("消息的路由routingKey错误 返回消息体为{}", returned);
    }
}
```

### **8.2.4.** **结果分析**

![image-20230711231725814](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230711231725814.png)

## **8.3.** **备份交换机**

有了 `mandatory` 参数和回退消息，我们获得了对无法投递消息的感知能力，有机会在生产者的消息无法被投递时发现并处理。但有时候，我们并不知道该如何处理这些无法路由的消息，最多打个日志，然后触发报警，再来手动处理。而通过日志来处理这些无法路由的消息是很不优雅的做法，特别是当生产者所在的服务有多台机器的时候，手动复制日志会更加麻烦而且容易出错。而且设置 `mandatory` 参数会增加生产者的复杂性，需要添加处理这些被退回的消息的逻辑。如果既不想丢失消息，又不想增加生产者的复杂性，该怎么做呢？前面在设置死信队列的文章中，我们提到，可以为队列设置死信交换机来存储那些处理失败的消息，可是这些不可路由消息根本没有机会进入到队列，因此无法使用死信队列来保存消息。在 `RabbitMQ` 中，有一种备份交换机的机制存在，可以很好的应对这个问题。什么是备份交换机呢？备份交换机可以理解为 `RabbitMQ` 中交换机的“备胎”，当我们为某一个交换机声明一个对应的备份交换机时，就是为它创建一个备胎，当交换机接收到一条不可路由消息时，将会把这条消息转发到备份交换机中，由备份交换机来进行转发和处理，通常备份交换机的类型为 Fanout ，这样就能把所有消息都投递到与其绑定的队列中，然后我们在备份交换机下绑定一个队列，这样所有那些原交换机无法被路由的消息，就会都进入这个队列了。当然，我们还可以建立一个报警队列，用独立的消费者来进行监测和报警。

### **8.3.1.** **代码架构图**

![image-20230711235230385](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230711235230385.png)

### **8.3.2.** **修改配置类** 

这里首先我们肯定要在之前的代码上增加备份交换机，备份队列，报警队列的配置和绑定关系。而普通交换机和备份交换机的绑定关系，就不能像之前一样直接`new`了，而是要使用交换机`Builder`。

```java
@Configuration
public class ConfirmConfig {

    // 普通交换机
    public static final String CONFIRM_EXCHANGE_NAME = "confirm.exchange";

    // 备份交换机
    public static final String BACKUP_EXCHANGE_NAME = "backup.exchange";

    // 队列名称
    public static final String CONFIRM_QUEUE_NAME = "confirm.queue";

    // 备份队列
    public static final String BACKUP_QUEUE_NAME = "backup.queue";

    // 报警队列
    public static final String WARNING_QUEUE_NAME = "warning.queue";

    // RoutingKey
    public static final String CONFIRM_ROUTE_KEY = "key1";

    // 普通交换机声明
    @Bean
    public DirectExchange confirmExchange() {
        return ExchangeBuilder
                .directExchange(CONFIRM_EXCHANGE_NAME)
                .durable(true)
                .alternate(BACKUP_EXCHANGE_NAME)
                .build();
    }

    // 备份交换机声明
    @Bean
    public FanoutExchange backupExchange() {
        return ExchangeBuilder
                .fanoutExchange(BACKUP_EXCHANGE_NAME)
                .durable(true)
                .build();
    }

    // 普通消费队列声明
    @Bean
    public Queue confirmQueue() {
        return QueueBuilder.durable(CONFIRM_QUEUE_NAME).build();
    }

    // 备份队列声明
    @Bean
    public Queue backupQueue() {
        return QueueBuilder.durable(BACKUP_QUEUE_NAME).build();
    }

    // 报警队列声明
    @Bean
    public Queue warningQueue() {
        return QueueBuilder.durable(WARNING_QUEUE_NAME).build();
    }

    // 普通路由队列绑定
    @Bean
    public Binding confirmQueueBindingConfirmExchange(
            @Qualifier("confirmExchange") DirectExchange directExchange,
            @Qualifier("confirmQueue") Queue confirmQueue) {

        return BindingBuilder.bind(confirmQueue).to(directExchange).with(CONFIRM_ROUTE_KEY);
    }

    // 备份交换机和备份队列绑定
    @Bean
    public Binding backupQueueBindingBackupExchange(
            @Qualifier("backupExchange") FanoutExchange backupExchange,
            @Qualifier("backupQueue") Queue backupQueue) {

        return BindingBuilder.bind(backupQueue).to(backupExchange);
    }

    // 备份交换机和报警队列绑定
    @Bean
    public Binding warningQueueBindingBackupExchange(
            @Qualifier("backupExchange") FanoutExchange backupExchange,
            @Qualifier("warningQueue") Queue warningQueue) {

        return BindingBuilder.bind(warningQueue).to(backupExchange);
    }
}
```

### **8.3.3.** **报警消费者** 

```java
@Component
@Slf4j
public class WarningConsumer {

    @RabbitListener
    public void receiveWarningMsg(Message message) throws Exception {
        String msg = new String(message.getBody(), "UTF-8");
        log.error("【报警消费者】发现路由错误的消息: {}", msg);
    }
}
```

### **8.3.4.** **测试注意事项** 

重新启动项目的时候需要把原来的 confirm.exchange 删除因为我们修改了其绑定属性，不然报以下错:

![image-20230712003214398](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712003214398.png)

### **8.3.5.** **结果分析** 

测试用例
> http://localhost:8080/confirm/sendConfirmMsg/你好1

![image-20230712005821026](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712005821026.png)

`mandatory` 参数与备份交换机可以一起使用的时候，如果两者同时开启，消息究竟何去何从？谁优先级高，经过上面结果显示答案是<font color="bb000">**备份交换机优先级高**。</font>

同时这里发现<font color="bb000">**【Ack回调接口】居然先于生产者处的发送消息的输出，可见回调接口是异步实现的。**</font>

# 9. RabbitMQ 其他知识点

## 9.1. **幂等性**

### 9.1.1. 概念

用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用。举个最简单例子，那就是支付，用户购买商品后支付，支付扣款成功，但是返回结果的时候网络异常，此时钱已经扣了，用户再点击按钮，此时会进行第二次扣款，返回结果成功，用户查询余额发现多扣钱了，流水记录也变成了两条。在以前的应用系统中，我们只需要把数据操作放入事务中即可，发生错误立即回滚，但是再响应客户端的时候也有可能出现网中断或者异常等等

### 9.1.2.消息重复消费

消费者在消费 MQ 中的消息时，MQ 已把消息发送给消费者，消费者在给 MQ 返回 ack 时网络中断，故 MQ 未收到确认信息，该条消息会重新发给其他的消费者，或者在网络重连后再次发送给该消费者，但实际上该消费者已成功消费了该条消息，造成消费者消费了重复的消息。

### **9.1.3.** **解决思路** 

**MQ 消费者的幂等性的解决一般使用全局 ID 或者写个唯一标识比如时间戳 或者 UUID 或者订单消费者消费 MQ 中的消息也可利用 MQ 的该 id 来判断，或者可按自己的规则生成一个全局唯一 id，每次消费消息时用该 id 先判断该消息是否已消费过。**

### **9.1.4.** **消费端的幂等性保障** 

在海量订单生成的业务高峰期，生产端有可能就会重复发生了消息，这时候消费端就要实现幂等性，这就意味着我们的消息永远不会被消费多次，即使我们收到了一样的消息。业界主流的幂等性有两种操作:**a.唯一 ID+指纹码机制,利用数据库主键去重, b.利用 redis 的原子性去实现**

### **9.1.5.** **唯一 ID+指纹码机制** 

指纹码:我们的一些规则或者时间戳加别的服务给到的唯一信息码,它并不一定是我们系统生成的，基本都是由我们的业务规则拼接而来，但是一定要保证唯一性，然后就利用查询语句进行判断这个 id 是否存在数据库中,优势就是实现简单就一个拼接，然后查询判断是否重复；劣势就是在高并发时，如果是单个数据库就会有写入性能瓶颈当然也可以采用分库分表提升性能，但也不是我们最推荐的方式。

### **9.1.6.** **Redis 原子性** 

利用 redis 执行 setnx 命令，天然具有幂等性。从而实现不重复消费

**setnx 锁缺乏可重入性，缺乏尝试机制，集群模式下缺乏主从一致性，一般使用reddison的分布式锁**

## **9.2.** **优先级队列**

## 9.2.1.**使用场景** 

在我们系统中有一个**订单催付**的场景，我们的客户在天猫下的订单,淘宝会及时将订单推送给我们，如果在用户设定的时间内未付款那么就会给用户推送一条短信提醒，很简单的一个功能对吧，但是，`tmall`商家对我们来说，肯定是要分大客户和小客户的对吧，比如像苹果，小米这样大商家一年起码能给我们创造很大的利润，所以理应当然，他们的订单必须得到优先处理，而曾经我们的后端系统是使用 redis 来存放的定时轮询，大家都知道 `redis` 只能用 `List` 做一个简简单单的消息队列，并不能实现一个优先级的场景，所以订单量大了后采用 `RabbitMQ` 进行改造和优化,如果发现是大客户的订单给一个相对比较高的优先级，否则就是默认优先级。

### **9.2.2.** **如何添加** 

![image-20230712011715000](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712011715000.png)

b.队列中代码添加优先级

```java
Map<String, Object> params = new HashMap();
params.put("x-max-priority", 10);
channel.queueDeclare("hello", true, false, false, params);
```

c.消息中代码添加优先级

```java
AMQP.BasicProperties properties = new AMQP.BasicProperties().builder().priority(5).build();
```

**<font color="bb000">d.注意事项</font>**

要让队列实现优先级需要做的事情有如下事情:队列需要设置为优先级队列，消息需要设置消息的优先级，消费者需要等待消息已经发送到队列中才去消费因为，这样才有机会对消息进行排序

### **9.2.3.** **实战** 

**这里老师使用原始包的版本，我这里直接写了`springboot`版本，使用普通路由【direct类型】，然后存在一个优先队列，同时我们为了能看到我们优先队列的效果，先注释掉消费者，然后先把消息存到队列里面，然后再解除注释并重启项目，完成消费的可视化。**

#### 1. 配置类

```java
@Slf4j
@Configuration
public class PriorityConfig {

    // 优先队列
    public static final String PRIORITY_QUEUE = "priority.queue";

    // 正常交换机
    public static final String NORMAL_EXCHANGE = "normal.exchange";

    // routingKey
    public static final String ROUTING_KEY = "key";

    // 声明正常交换机
    @Bean
    public DirectExchange normalExchange(){
        return ExchangeBuilder.directExchange(NORMAL_EXCHANGE).durable(true).build();
    }

    // 声明优先队列
    @Bean
    public Queue priorityQueue(){
        return QueueBuilder.durable(PRIORITY_QUEUE).maxPriority(10).build();
    }

    // 绑定关系
    @Bean
    public Binding priorityQueueBindingNormalExchange(
            @Qualifier("normalExchange") DirectExchange normalExchange,
            @Qualifier("priorityQueue") Queue priorityQueue){
        return BindingBuilder.bind(priorityQueue).to(normalExchange).with(ROUTING_KEY);
    }
}
```

#### 2. 生产者

```java
@RestController
@Slf4j
@RequestMapping("/priority")
public class PriorityController {

    @Autowired
    private RabbitTemplate rabbitTemplate;

    @GetMapping("/sendPriorityMsg/{message}/{priority}")
    public void sendPriorityMsg(@PathVariable("message") String message,
                                @PathVariable("priority") Integer priority){

        log.info("【浏览器端】生产者给优先队列放入消息: {}, 它的优先级为: {}", message, priority);

        rabbitTemplate.convertAndSend(
                PriorityConfig.NORMAL_EXCHANGE,
                PriorityConfig.ROUTING_KEY,
                message,
                msg -> {
                    msg.getMessageProperties().setPriority(priority);
                    return msg;
                });
    }
}
```

#### 3. 消费者

```java
@Slf4j
@Component
public class PriorityConsumer {

    @RabbitListener(queues = PriorityConfig.PRIORITY_QUEUE)
    public void receivePriorityMsg(Message message) throws Exception {
        Integer priority = message.getMessageProperties().getPriority();
        String msg = new String(message.getBody(), "UTF-8");
        log.info("【消费者端】消息优先级: {} 消息为: {}", priority, msg);
    }
}
```

#### 4. 测试和结果

**我们为了能看到我们优先队列的效果，先注释掉消费者，然后先把消息存到队列里面，然后再解除注释并重启项目，完成消费的可视化。**

> localhost:8080/priority/sendPriorityMsg/消息1/4
>
> localhost:8080/priority/sendPriorityMsg/消息2/2
>
> localhost:8080/priority/sendPriorityMsg/消息3/3
>
> localhost:8080/priority/sendPriorityMsg/消息4/9

显然，这里如果实现了优先队列的消费，我们应该是效果是消费先后顺序为 ：

**<font color="bb000">消息4 -> 消息1 -> 消息3 -> 消息2</font>**

这里我们的回调显示是因为之前做消息应答的时候添加的不用管。

![image-20230712020711532](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712020711532.png)

重启后果然，和我们设想的消费顺序一致

![image-20230712021206112](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712021206112.png)

## **9.3.** **惰性队列**

### **9.3.1.** **使用场景** 

`RabbitMQ` 从 `3.6.0` 版本开始引入了惰性队列的概念。惰性队列会尽可能的将消息存入磁盘中，而在消费者消费到相应的消息时才会被加载到内存中，它的一个重要的设计目标是能够支持更长的队列，即支持更多的消息存储。当消费者由于各种各样的原因(比如消费者下线、宕机亦或者是由于维护而关闭等)而致使长时间内不能消费消息造成堆积时，惰性队列就很有必要了。默认情况下，当生产者将消息发送到 `RabbitMQ` 的时候，队列中的消息会尽可能的存储在内存之中，这样可以更加快速的将消息发送给消费者。即使是持久化的消息，在被写入磁盘的同时也会在内存中驻留一份备份。当 `RabbitMQ` 需要释放内存的时候，会将内存中的消息换页至磁盘中，这个操作会耗费较长的时间，也会阻塞队列的操作，进而无法接收新的消息。虽然 `RabbitMQ` 的开发者们一直在升级相关的算法，但是效果始终不太理想，尤其是在消息量特别大的时候。

### **9.3.2.** 两种模式

队列具备两种模式：`default` 和 `lazy`。默认的为 `default` 模式，在 `3.6.0` 之前的版本无需做任何变更。`lazy`模式即为惰性队列的模式，可以通过调用 `channel.queueDeclare`方法的时候在参数中设置，也可以通过`Policy` 的方式设置，如果一个队列同时使用这两种方式设置的话，那么 `Policy` 的方式具备更高的优先级。如果要通过声明的方式改变已有队列的模式的话，那么只能先删除队列，然后再重新声明一个新的。在队列声明的时候可以通过“`x-queue-mode`”参数设置队列的模式，取值为“`default`”和“`lazy`”。下面示例中演示了一个惰性队列的声明细节：

```java
Map<String, Object> args = new HashMap<String, Object>();

args.put("x-queue-mode", "lazy");

channel.queueDeclare("myqueue", false, false, false, args);
```

### **9.3.3.** **内存开销对比** 

![image-20230712021540731](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712021540731.png)

**在发送 1 百万条消息，每条消息大概占 1KB 的情况下，普通队列占用内存是 1.2GB，而惰性队列仅仅占用 1.5MB**

# **10. RabbitMQ** **集群**

## **10.1. clustering**

### **10.1.1. 使用集群的原因** 

最开始我们介绍了如何安装及运行 `RabbitMQ` 服务，不过这些是单机版的，无法满足目前真实应用的要求。如果`RabbitMQ` 服务器遇到内存崩溃、机器掉电或者主板故障等情况，该怎么办？单台 `RabbitMQ` 服务器可以满足每秒`1000` 条消息的吞吐量，那么如果应用需要 `RabbitMQ` 服务满足每秒 `10` 万条消息的吞吐量呢？购买昂贵的服务器来增强单机 `RabbitMQ` 务的性能显得捉襟见肘，搭建一个 `RabbitMQ` 集群才是解决实际问题的关键.

### **10.1.2. 搭建步骤** 

搭建集群，三台虚拟机分别都安装`rabbitmq`，并且在 `/etc/hosts`根据彼此的`ip`写一个方便记忆的域名。

`rabbitmq`集群需要各个节点的`cookie`文件使用同一个值

在 第一台服务器 上执行远程操作命令

```shell
scp /var/lib/rabbitmq/.erlang.cookie root@hadoop101:/var/lib/rabbitmq/.erlang.cookie
```

```shell
scp /var/lib/rabbitmq/.erlang.cookie root@hadoop102:/var/lib/rabbitmq/.erlang.cookie
```

启动 `RabbitMQ` 服务,顺带启动 `Erlang` 虚拟机和 `RbbitMQ` 应用服务(在三台节点上分别执行以下命令)【这个命令就可以完成`Erlang`虚拟机和`RabbitMQ`的重启】

```shell
rabbitmq-server -detached
```

这里如果报错Error when reading /var/lib/rabbitmq/...错误，是由于Cookie的权限问题，也就是rabbit这个属性组不认识这个Cookie，使用下面的命令解决

> chown rabbitmq:rabbitmq Cookie路径

在节点 2 执行

```shell
rabbitmqctl stop_app
```

(`rabbitmqctl stop` 会将 `Erlang` 虚拟机关闭，`rabbitmqctl stop_app` 只关闭 `RabbitMQ` 服务)

```shell
rabbitmqctl reset
```

```shell
rabbitmqctl join_cluster rabbit@hadoop100
```

```shell
rabbitmqctl start_app
```

(只启动应用服务)

在节点 3 执行

```sh
rabbitmqctl stop_app
```

```sh
rabbitmqctl reset
```

```sh
rabbitmqctl join_cluster rabbit@hadoop100
```

```sh
rabbitmqctl start_app
```

集群状态

```sh
rabbitmqctl cluster_status
```

解除集群节点的命令(node2 和 node3 机器分别执行)

```sh
rabbitmqctl stop_app
```

```sh
rabbitmqctl reset
```

```sh
rabbitmqctl start_app
```

```sh
rabbitmqctl cluster_status
```

(node1 机器上执行) 

```sh
rabbitmqctl forget_cluster_node rabbit@hadoop101
```

## **10.2.** **镜像队列**

### **10.2.1. 使用镜像的原因** 

如果 `RabbitMQ` 集群中只有一个 `Broker` 节点，那么该节点的失效将导致整体服务的临时性不可用，并且也可能会导致消息的丢失。可以将所有消息都设置为持久化，并且对应队列的`durable`属性也设置为`true`，但是这样仍然无法避免由于缓存导致的问题：因为消息在发送之后和被写入磁盘井执行刷盘动作之间存在一个短暂却会产生问题的时间窗。通过 `publisherconfirm` 机制能够确保客户端知道哪些消息己经存入磁盘，尽管如此，一般不希望遇到因单点故障导致的服务不可用。

引入镜像队列(`Mirror Queue`)的机制，可以将队列镜像到集群中的其他 `Broker` 节点之上，如果集群中的一个节点失效了，队列能自动地切换到镜像中的另一个节点上以保证服务的可用性。

### **10.2.2. 搭建步骤** 

1. 启动三台集群节点

2. 随便找一个节点添加 policy【点击admin然后点右侧的policy】

   名字是随便取，见名知意即可，这里是给 / 添加策略

   第二行pattern是正则匹配，这里填写代表必须以`mirrior`开头才行

   `HA` 全称`high available` 高可用

   设置匹配数为 `2`【代表本身一份，还有一份放在别的集群节点，且能在宕机后自动转移还保持两份在两个不同的节点】

   同步策略为自动

![image-20230712202242721](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712202242721.png)

3. 在 node1 上创建一个队列发送一条消息，队列存在镜像队列

![image-20230712202256950](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712202256950.png)

4. 停掉 node1 之后发现 node2 成为镜像队列

```sh
rabbitmqctl stop_app
```

![image-20230712202333993](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712202333993.png)

5. 就算整个集群只剩下一台机器了 依然能消费队列里面的消息

 说明队列里面的消息被镜像队列传递到相应机器里面了

## **10.3. Haproxy+Keepalive** **实现高可用负载均衡**

### **10.3.1. 整体架构图** 

![image-20230712023335339](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712023335339.png)

### **10.3.2. Haproxy 实现负载均衡** 

`HAProxy` 提供高可用性、负载均衡及基于 `TCPHTTP` 应用的代理，支持虚拟主机，它是免费、快速并且可靠的一种解决方案，包括 `Twitter`,`Reddit`,`StackOverflow`,`GitHub` 在内的多家知名互联网公司在使用。`HAProxy` 实现了一种事件驱动、单一进程模型，此模型支持非常大的井发连接数。

扩展 `nginx`,`lvs`,`haproxy` 之间的区别: http://www.ha97.com/5646.html

### **10.3.3. 搭建步骤**

1. 下载 `haproxy`(在 `node1` 和 `node2`)

```sh
yum -y install haproxy
```

2. 修改 `node1` 和 `node2` 的 `haproxy.cfg`

```sh
vim /etc/haproxy/haproxy.cfg
```

需要修改红色 IP 为当前机器 IP

![image-20230712205023724](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712205023724.png)

3. 在两台节点启动 `haproxy`

```sh
haproxy -f /etc/haproxy/haproxy.cfg
```

```sh
ps -ef | grep haproxy
```

4. 访问地址

http://localhost:8888/stats

### **10.3.4. Keepalived 实现双机(主备)热备** 

试想如果前面配置的 `HAProxy` 主机突然宕机或者网卡失效，那么虽然 `RbbitMQ` 集群没有任何故障但是对于外界的客户端来说所有的连接都会被断开结果将是灾难性的为了确保负载均衡服务的可靠性同样显得十分重要，这里就要引入 `Keepalived` 它能够通过自身健康检查、资源接管功能做高可用(双机热备)，实现故障转移

### **10.3.5. 搭建步骤** 

1. 下载 `keepalived`

```sh
yum -y install keepalived
```

2. 节点 node1 配置文件

```sh
vim /etc/keepalived/keepalived.conf
```

把资料里面的 `keepalived.conf` 修改之后替换

3. 节点 node2 配置文件

需要修改 `global_defs` 的 `router_id`,如:`nodeB`

其次要修改 `vrrp_instance_VI` 中 `state` 为"`BACKUP`"；

最后要将 `priority` 设置为小于 `100` 的值

4. 添加 `haproxy_chk.sh`

(为了防止 HAProxy 服务挂掉之后 Keepalived 还在正常工作而没有切换到 Backup 上，所以这里需要编写一个脚本来检测 HAProxy 务的状态,当 HAProxy 服务挂掉之后该脚本会自动重启

HAProxy 的服务，如果不成功则关闭 Keepalived 服务，这样便可以切换到 Backup 继续工作)

```sh
vim /etc/keepalived/haproxy_chk.sh
```

(可以直接上传文件)

修改权限 

```sh
chmod 777 /etc/keepalived/haproxy_chk.sh
```

5.启动 keepalive 命令(node1 和 node2 启动)

```sh
systemctl start keepalived
```

6.观察 Keepalived 的日志

```sh
tail -f /var/log/messages -n 200
```

7.观察最新添加的 vip

```sh
ip add show
```

8.node1 模拟 keepalived 关闭状态

```sh
systemctl stop keepalived 
```

9.使用 vip 地址来访问 rabbitmq 集群

## **10.4. Federation Exchange**

### **10.4.1. 使用它的原因** 

(broker 北京)，(broker 深圳)彼此之间相距甚远，网络延迟是一个不得不面对的问题。有一个在北京的业务(Client 北京) 需要连接(broker 北京)，向其中的交换器 exchangeA 发送消息，此时的网络延迟很小，(Client 北京)可以迅速将消息发送至 exchangeA 中，就算在开启了 publisherconfirm 机制或者事务机制的情况下，也可以迅速收到确认信息。此时又有个在深圳的业务(Client 深圳)需要向 exchangeA 发送消息，那么(Client 深圳) (broker 北京)之间有很大的网络延迟，(Client 深圳) 将发送消息至 exchangeA 会经历一定的延迟，尤其是在开启了 publisherconfirm 机制或者事务机制的情况下，(Client 深圳) 会等待很长的延迟时间来接收(broker 北京)的确认信息，进而必然造成这条发送线程的性能降低，甚至造成一定程度上的阻塞。

将业务(Client 深圳)部署到北京的机房可以解决这个问题，但是如果(Client 深圳)调用的另些服务都部署在深圳，那么又会引发新的时延问题，总不见得将所有业务全部部署在一个机房，那么容灾又何以实现？这里使用 Federation 插件就可以很好地解决这个问题.

![image-20230712023625470](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712023625470.png)

### **10.4.2. 搭建步骤** 

1.需要保证每台节点单独运行

2.在每台机器上开启 federation 相关插件【这两个插件自带，但是没有启动】

```sh
rabbitmq-plugins enable rabbitmq_federation
```

```sh
rabbitmq-plugins enable rabbitmq_federation_management
```

在admin的右侧栏可以看到多了这两个选项

![image-20230712205657823](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712205657823.png)

3. 原理图(先运行 consumer 在 node2 创建 fed_exchange)

![image-20230712205717015](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712205717015.png)

4. 在 downstream(node2)配置 upstream(node1)

![image-20230712205738497](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712205738497.png)

4. 添加 policy

![image-20230712205759964](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712205759964.png)

5. 成功的前提

![image-20230712205814908](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712205814908.png)

## **10.5. Federation Queue**

### **10.5.1. 使用它的原因** 

联邦队列可以在多个 `Broker` 节点(或者集群)之间为单个队列提供均衡负载的功能。一个联邦队列可以连接一个或者多个上游队列(`upstream queue`)，并从这些上游队列中获取消息以满足本地消费者消费消息的需求。

### **10.5.2. 搭建步骤** 

箭头没反，联邦的方向和消息走向是相反的

![image-20230712023719647](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712023719647.png)

2. 添加 upstream(同上)

3. 添加 policy

![image-20230712210716937](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712210716937.png)

## **10.6. Shovel**

### **10.6.1. 使用它的原因** 

`Federation` 具备的数据转发功能类似，`Shovel` 够可靠、持续地从一个 `Broker` 中的队列(作为源端，即`source`)拉取数据并转发至另一个 `Broker` 中的交换器(作为目的端，即 `destination`)。作为源端的队列和作为目的端的交换器可以同时位于同一个 `Broker`，也可以位于不同的 `Broker` 上。`Shovel` 可以翻译为"铲子"，是一种比较形象的比喻，这个"铲子"可以将消息从一方"铲子"另一方。`Shovel` 行为就像优秀的客户端应用程序能够负责连接源和目的地、负责消息的读写及负责连接失败问题的处理。

### **10.6.2. 搭建步骤** 

1. 开启插件(需要的机器都开启)

```sh
rabbitmq-plugins enable rabbitmq_shovel
```

```sh
rabbitmq-plugins enable rabbitmq_shovel_management
```

![image-20230712210804239](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712210804239.png)

2.原理图(在源头发送的消息直接回进入到目的地队列)

![image-20230712210820060](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712210820060.png)

3. 添加 shovel 源和目的地 

![image-20230712210835347](C:\Users\86177\AppData\Roaming\Typora\typora-user-images\image-20230712210835347.png)
